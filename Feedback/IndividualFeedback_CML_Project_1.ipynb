{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedback: CML Project 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmgT1MrOzSF2"
      },
      "source": [
        "# **Feedback: Extended Linear Models prediction project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5GdYF1zPVl"
      },
      "source": [
        "| Criteria | Weighting (%) | Score | Justification | Improvements  |\n",
        "|----------|---------------|-------|---------------|---------------|\n",
        "|Code runs | 10            |  10    |              |               |\n",
        "|Data preparation |\t30 | 25 | Standardisation done correctly, dummy categories managed, training and testing columns matched and some nice polynomial feature investigation done. Bespoke imputation for numerical and categorical features was nice, although it is preferable to use the training set summaries to impute the \n",
        "|Linear Models(s) have been used | 10 | 10 |   | \n",
        "|LASSO has been used  | 10 | 10 | \n",
        "|Parcel value prediction made  | 10 | 10  | \n",
        "|Accuracy | 5 | 4 | \n",
        "|Hyperparameter optimization | 10 | 10 | Done using AIC to avoid computational burden  | \n",
        "|Neat code with titles and comments | 5 | 5 | Very good commentary of code, methods and results\n",
        "|Improved methods from those discussed in class | 10 | 3 | AIC hyperparameter selection and bespoke imputers for categorical and continuous variables\n",
        "\n",
        "Score: 87\n",
        "\n",
        "Feedback: Really nice work. Good understanding of preparing a data set for sklearn task. Standardisation correct, dummies correctly managed and testing and training columns matched. Imputation of numerical and categorical variables separately as nice, but should use the training summaries rather than the independent testing ones. Good understanding of linear regression and LASSO methods with hyperparameter selection using the AIC.\n",
        "\n",
        "Notes:\n",
        "You are right that the LASSO will be less vulnerable to outliers than LinearRegression, but LASSO is still fairly vulnerable, this si the ature of the least squares loss function.\n",
        "\n",
        "What you say about plf(2) vs plf(3) hit at the friction between machine learning and statistics/economics. Machine Learning says if you reduce the out of sample score it's better (you may have got an improved accuracy mark had you done this for example) but as you correctly say if you want to be able to explain your model then so doing will not always work"
      ]
    }
  ]
}